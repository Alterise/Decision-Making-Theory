\documentclass{article}

\usepackage{cmap}					
\usepackage{mathtext} 			
\usepackage{amssymb}
\usepackage[T2A]{fontenc}			
\usepackage[utf8]{inputenc}	
\usepackage[english,russian]{babel}	
\usepackage{indentfirst}
\frenchspacing

\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{Курсовая Работа по ТПР}
\author{Терешков Алексей, Алимов Исмаил, Мамченков Дмитрий}
\date{}

\begin{document}
\maketitle

\section{Задача}
Рассмотрим алгоритм экспоненциального взвешивания на основе градиента с переменным шагом обучения. Приведите исчерпывающие выкладки и получите оценку на кумулятивный регрет. Как следует выбирать параметр обучения на каждом шаге?

\section{Решение}
Рассмотрим алгоритм экспоненциального взвешивания на основе градиента с переменным шагом обучения. Будем предполагать, что функция потерь $l(x, y)$ является не только выпуклой по первому аргументу, но и дифференцируемой, а ее градиент по x будем обозначать через $\nabla l(x, y)$. Выбор оптимального параметра $\varepsilon$ (шаг обучения) возможен только при априорном знании количества раундов $T$. Вместе с тем особый интерес представляют алгоритмы, для которых справедлива оценка $R_{k,t} =o(t)$, т. е. средний относительно числа раундов кумулятивный регрет стремится к нулю:

\[
\frac{R_{k,t}}{t} \rightarrow 0 \quad при \quad t \rightarrow +\infty
\]

Для решения этой проблемы параметр $\varepsilon$ можно выбирать в зависимости от раунда $t$, т. е. $\varepsilon = \varepsilon_t$.

В такой постановке предсказание алгоритмом экспоненциального взвешивания на основе градиента  с переменным шагом обучения осуществляется по формуле

\[
\hat{p}_t = \frac{ \sum^N_{k=1} exp[-\varepsilon_t \sum^{t-1}_{s=1} \langle \nabla l(\hat{p}_s, y_s), f_{k,t} \rangle ] f_{k,t} }{ \sum^N_{k=1} exp[-\varepsilon_t \sum^{t-1}_{s=1} \langle \nabla l(\hat{p}_s, y_s), f_{k,t} \rangle ] }
\]

Докажем следующее утверждение.

Пусть пространство решений $D$ является выпуклым подмножеством единичного евклидова шара $\{ u \in \mathbb{R} ^d : \lvert \lvert  u\lvert \lvert_2 \le 1\}$ и функция потерь $l(\cdot, \cdot)$ является
выпуклой по первому аргументу, а ее градиент удовлетворяет неравенству

\[
\lvert \lvert \nabla l(u, y) \lvert \lvert_2 \le 1
\]

для $u \in D, y \in Y$. Тогда для любого раунда $t$ и $\varepsilon_t > 0$, а также произвольных исходов
$\{y_s\}^t_{s=1} \subset Y$ справедлива следующая оценка на куммулятивный регрет:

\[
\hat{L}_t - L_{k,t} \le \frac{\ln{N}}{\varepsilon_t} + \frac{\varepsilon_t t}{2}.
\]

Доказательство. Веса в этом алгоритме определяется формулой

\[
w_k(t-1) = exp[-\varepsilon_t \sum^{t-1}_{s=1} \langle \nabla l(\hat{p}_s, y_s), f_{k,s} \rangle].
\]

Введем новую функцию потерь по закону

\[
l_*(u, y_s) = \langle \nabla l(\hat{p}_s, y_s), u \rangle, \quad u \in D.
\]

В силу неравенства Коши – Буняковского $l_*(u, y_s) \in [-1, 1]$, поэтому чтобы применить
результат об оптимальной оценке алгоритма экспоненциального взвешивания, введем
еще одну функцию

\[
l_\circ (u, y_s) = \frac{1 + l_*(u, y_s)}{2},
\]

принимающую значения из отрезка $[0, 1]$. Тогда

\[
\frac{1}{2} \sum^t_{s=1} \langle \hat{p}_s - f_{k,s}, \nabla l(\hat{p}_s, y_s) \rangle = \frac{1}{2} \sum^t_{s=1} l_*(\hat{p}_s, y_s) - \frac{1}{2} \sum^t_{s=1} l_*(f_{k,s}, y_s) = \frac{1}{2} \sum^t_{s=1} l_\circ(\hat{p}_s, y_s) - \frac{1}{2} \sum^t_{s=1} l_\circ(f_{k,s}, y_s) \le \frac{\ln{N}}{2 \varepsilon_t} + \frac{\varepsilon_t t}{4},
\]

поскольку для функции потерь $l_\circ(\cdot, \cdot)$ обычный алгоритм экспоненциального взвешивания в нашем случае имеет параметр $2 \varepsilon_t$. Вспоминая, что функция $l(\cdot, \cdot)$ является выпуклой по первому аргументу, записываем неравенство

\[
l(f_{k,s}, y_s) \ge l(\hat{p}_s, y_s) + \langle f_{k, s} - \hat{p}_s, \nabla l(\hat{p}_s, y_s) \rangle
\]
откуда 

\[
\langle \hat{p}_s - f_{k, s}, \nabla l(\hat{p}_s, y_s) \rangle \ge l(\hat{p}_s, y_s) - l(f_{k,s}, y_s).
\]

Таким образом,

\[
\hat{L}_t - L_{k,t} \le \langle \hat{p}_s - f_{k, s}, \nabla l(\hat{p}_s, y_s) \rangle \le \frac{ln{N}}{\varepsilon_t} + \frac{\varepsilon_t t}{2}.
\]

\end{document}
