\documentclass{article}

\usepackage{cmap}					
\usepackage{mathtext} 				
\usepackage[T2A]{fontenc}			
\usepackage[utf8]{inputenc}	
\usepackage[english,russian]{babel}	
\usepackage{indentfirst}
\frenchspacing

\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{Курсовая Работа по ТПР}
\author{Терешков Алексей, Алимов Исмаил, Мамченков Дмитрий}
\date{}

\begin{document}
\maketitle

\section{Задача}
Рассмотрим алгоритм экспоненциального взвешивания на основе градиента с переменным шагом обучения. Приведите исчерпывающие выкладки и получите оценку на кумулятивный регрет. Как следует выбирать параметр обучения на каждом шаге?

\section{Решение}
В алгоритме экспоненциального взвешивания экспертных решений оптимальная
оценка на кумулятивный регрет имеет вид

\[
R_{k,t} \le \frac{\ln{N}}{\varepsilon} + \frac{\varepsilon t}{8} = O(t) 
\]

а выбор оптимального параметра $\varepsilon$ возможен только при априорном знании количества раундов $T$. \\

Для решения этой проблемы параметр $\varepsilon$ можно выбирать в зависимости от раунда $t$, т. е. $\varepsilon = \varepsilon_t$. В такой постановке предсказание алгоритмом экспоненциального взвешивания экспертных решений с переменным параметром $\varepsilon_t$ осуществляется по формуле:

\[
\hat{p_t} = \frac{\sum^N_{k=1} e^{-\varepsilon_tL_{k,t-1}} f_{k,t} }{\sum^N_{k=1} e^{-\varepsilon_tL_{k,t-1}}} 
\]

Предположим, что функция потерь $l(\cdot, \cdot)$ является выпуклой по первому аргументу и принимает значения из отрезка [0, 1]. Тогда для любой монотонно убывающей последовательности $\{\varepsilon_s\}_{s\in N}$ положительных чисел, для любого раунда $t \ge 1$, а также произвольных исходов $\{y_s\}^t_{s=1}$ кумулятивный регрет для алгоритма экспоненциального взвешивания с переменным параметром $\varepsilon_t$ удовлетворяет неравенству

\[
R_{k,t} \le \frac{\ln{N}}{\varepsilon_t} + \frac{1}{8} \sum^t_{s=1} \varepsilon_s.
\]

Доказательство: \\

Из выпуклости функции потерь $l(\cdot, \cdot)$ по первому аргументу следует неравенство

\[ 
l(\hat{p_t}, y_t) = l(\sum^N_{k=1} \frac{w_k(t-1)}{W_{t-1}} f_{k,t}, y_t) \le \sum^N_{k=1} \frac{w_k(t-1)}{W_{t-1}} l(f_{k,t}, y_t). 
\]

Далее, воспользуемся неравенством Хёфдинга, сразу применяя к левой и правой части функцию $\xi \mapsto e^{\varepsilon_t \xi}$, а затем еще и предыдущим неравенством:

\[ 
\sum^N_{k=1} \frac{w_k(t-1)}{W_{t-1}} exp[-\varepsilon_t \cdot l(f_{k,t}, y_t)] \le 
exp[ - \varepsilon_t \sum^N_{k=1} \frac{w_k(t-1)}{W_{t-1}} l(f_{k,t}, y_t) + \frac{\varepsilon^2_t}{8} ] \le
exp[-\varepsilon_t l(\hat{p_t}, y_t) + \frac{\varepsilon^2_t}{8}]
\]

Умножив левую и правую часть последнего неравенства на $ e^{\varepsilon_t l(\hat{p_t}, y_t) + \frac{\varepsilon^2_t}{8}}$ в итоге получим

\begin{equation}
\sum^N_{k=1} \frac{w_k(t-1)}{W_{t-1}} exp[-\varepsilon_t \cdot l(f_{k,t}, y_t) - \frac{\varepsilon^2_t}{8} + \varepsilon_t l(\hat{p_t}, y_t) ] \le 1.
\end{equation}

Введем вспомогательные величины

\[  
a_k(t-1) = exp[ -\varepsilon_{t-1} L_{k,t-1} + \varepsilon_{t-1} (\hat{L}_{t-1} - \frac{1}{8} \sum^{t-1}_{s=1} \varepsilon_s)],
\]

заметив, что 

\begin{equation}
\frac{w_k(t-1)}{W_{t-1}} = \frac{ \frac{1}{N} [a_k(t-1)]^{\varepsilon_t / \varepsilon_{t-1}} }{ \sum^N_{j=1} \frac{1}{N} [a_j(t - 1)]^{\varepsilon_t / \varepsilon_{t-1}} }.
\end{equation}

Используя принцип математической индукции покажем, что

\begin{equation}
\sum^N_{j=1} \frac{1}{N} a_j(t) \le 1.
\end{equation}

Очевидно, что $a_j (0) = 1$, поэтому при $t = 0$ неравенство (3) справедливо. Пусть теперь выполняется неравенство (3) при $t := t - 1$:

\[ 
\sum^N_{j=1} \frac{1}{N} a_j(t-1) \le 1.
\]

Поскольку $\varepsilon_{t-1} \ge \varepsilon_t > 0$, то функция $\xi \mapsto \xi^{\varepsilon_t / \varepsilon_{t-1}}$ является вогнутой, поэтому 

\[ 
\sum^N_{j=1} \frac{1}{N} [a_j(t-1)]^{\varepsilon_t / \varepsilon_{t-1}} \le [\sum^N_{j=1} \frac{1}{N} a_j(t-1)]^{\varepsilon_t / \varepsilon_{t-1}} \le 1.
\]

Используя полученную выше оценку, получим неравенство

\[
\frac{w_k(t-1)}{W_{t-1}} \ge \frac{1}{N} [a_j(t-1)]^{\varepsilon_t / \varepsilon_{t-1}},
\]

которое вместе с (1) приводит к неравенству

\[
\sum^N_{k=1} \frac{1}{N} [a_k(t-1)]^{\varepsilon_t / \varepsilon_{t-1}} exp [-\varepsilon_t \cdot l(f_{k,t}, y_t) - 
\frac{\varepsilon^2_t}{8} + \varepsilon_t \cdot l(\hat{p_t}, y_t) ] \le 1.
\]

Вместе с тем простая проверка показывает, что
 
\[
a_k(t) = [a_k(t-1)]^{\varepsilon_t / \varepsilon_{t-1}} exp [-\varepsilon_t \cdot l(f_{k,t}, y_t) - 
\frac{\varepsilon^2_t}{8} + \varepsilon_t \cdot l(\hat{p_t}, y_t) ].
\]

следовательно последнее неравенство и есть (3). Наконец, в силу неравенста (3) справедливы выкладки

\[
1 \ge \sum^N_{k=1} \frac{1}{N} a_k(t) \ge \frac{1}{N} a_k(t) = \frac{1}{N}  exp [-\varepsilon_t L_{k,t} + \varepsilon_t (\hat{L_t} - \frac{1}{8} \sum^t_{s=1} \varepsilon_s )],
\]

откуда после логарифмирования и упрощения получаем итоговое неравенство

\[
\hat{L_t} - L_{k,t} \le \frac{\ln{N}}{\varepsilon_t} + \frac{1}{8} \sum^t_{s=1} \varepsilon_s.
\]

\end{document}
